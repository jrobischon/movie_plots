{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jrobi\\Miniconda3\\envs\\tf_venv\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, LSTM, Dense, Bidirectional,\\\n",
    "                                     concatenate, Flatten, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wiki_movie_plots_deduped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**\n",
    "\n",
    "<img src=\"img/model_architecture.jpg\" width=400 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(x):\n",
    "    x = x.lower()\n",
    "    s_tokens =  sent_tokenize(x)\n",
    "    tokens = [TreebankWordTokenizer().tokenize(s) for s in s_tokens]\n",
    "    tokens = [[w for w in s if re.match(\"[A-Za-z]\", w) is not None] for s in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each movie plot into a list of sentences, each containing a list of tokens\n",
    "# Return tuple containing (list of tokens, movie index)\n",
    "X_plots = []\n",
    "for i, p in enumerate(df[\"Plot\"]):\n",
    "    sentences = tokenize_text(p)\n",
    "    for s in sentences:\n",
    "        X_plots.append((s, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate plots from movie index\n",
    "x, _ = zip(*X_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = Word2Vec(x, size=200, iter=10)\n",
    "wordvec.save(\"../models/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 0.8121660947799683),\n",
       " ('demon', 0.7041882276535034),\n",
       " ('beast', 0.6551673412322998),\n",
       " ('monstrous', 0.6548070907592773),\n",
       " ('sphere', 0.6453431248664856),\n",
       " ('dinosaur', 0.6338607668876648),\n",
       " ('mummy', 0.6314750909805298),\n",
       " ('werewolf', 0.6305549740791321),\n",
       " ('whale', 0.6272330284118652),\n",
       " ('giant', 0.6252624988555908)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.wv.most_similar(\"monster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6824626922607422),\n",
       " ('princess', 0.6508470177650452),\n",
       " ('countess', 0.5387574434280396),\n",
       " ('emperor', 0.5184895992279053),\n",
       " ('empress', 0.5171738266944885),\n",
       " ('prince', 0.49930691719055176),\n",
       " ('consort', 0.49101826548576355),\n",
       " ('goddess', 0.4869394898414612),\n",
       " ('dowager', 0.4809301197528839),\n",
       " ('clementianna', 0.47523051500320435)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of indices for all terms in vocab\n",
    "word2index = {\"<UNK>\": 0}\n",
    "for i, k in enumerate(wordvec.wv.index2word):\n",
    "    word2index[k] = i + 1\n",
    "    \n",
    "word_vectors = np.zeros((1, wordvec.wv.vectors.shape[1]))\n",
    "word_vectors = np.concatenate([word_vectors, wordvec.wv.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 59607\n",
      "Embedding matrix shape: (59607, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: %i\" %len(word2index))\n",
    "print(\"Embedding matrix shape: %s\" %str(word_vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_lookup(x):\n",
    "    try:\n",
    "        return word2index[x]\n",
    "    except KeyError:\n",
    "        return word2index[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plots_ind = [([index_lookup(x) for x in sentence], i) for sentence, i in X_plots]\n",
    "X_plots_ind = np.array(X_plots_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create generator for training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size = 32, n_neg = 5, max_len = 100):\n",
    "    num_samples = samples.shape[0]\n",
    "    \n",
    "    ind = np.arange(num_samples)\n",
    "\n",
    "    while True:\n",
    "        samples = sk_shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            X_sent = []\n",
    "            X_movie = []\n",
    "            y_out = [] \n",
    "            \n",
    "            # Sample positive examples\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            \n",
    "            batch_n = len(batch_samples)  # Size of current batch\n",
    "            \n",
    "            sentences = batch_samples[:, 0]\n",
    "            movie_indices = batch_samples[:, 1]\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([1]*batch_n)\n",
    "            \n",
    "            # Sample negative examples\n",
    "            keep_indx =  np.random.choice(ind, batch_n*n_neg, replace=False)\n",
    "            neg_samples = samples[keep_indx]\n",
    "            sentences = neg_samples[:, 0]\n",
    "            movie_indices = np.repeat(movie_indices, n_neg)\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([0]*(batch_n*n_neg))\n",
    "             \n",
    "            # Pad zeros\n",
    "            X_sent = pad_sequences(X_sent, maxlen=max_len)\n",
    "             \n",
    "            yield [X_sent.reshape(-1, max_len), np.array(X_movie).reshape(-1, 1)], np.array(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NEG_SAMPLE = 5\n",
    "train_generator = generator(X_plots_ind, batch_size = BATCH_SIZE, n_neg=NEG_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1536, 100), (1536, 1), (1536,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(train_generator)\n",
    "\n",
    "X[0].shape, X[1].shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "# Define query embedding layer\n",
    "d1, d2 = word_vectors.shape\n",
    "\n",
    "# Define query embedding layer\n",
    "query_embedding = Embedding(d1, d2, \n",
    "                            input_length = MAX_LEN,\n",
    "                            weights = [word_vectors],\n",
    "                            trainable = False)\n",
    "\n",
    "\n",
    "# Define movie embedding layer\n",
    "movie_embedding = Embedding(df.shape[0], d2,\n",
    "                            input_length = 1,\n",
    "                            trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "in1 = Input(shape=(MAX_LEN,))\n",
    "in2 = Input(shape=(1,))\n",
    "\n",
    "q = query_embedding(in1)\n",
    "q = LSTM(50)(q)\n",
    "q = Dense(500)(q)\n",
    "q = Dense(200)(q)\n",
    "\n",
    "m = movie_embedding(in2)\n",
    "m = Flatten()(m)\n",
    "\n",
    "c = dot([q, m], axes = 1, normalize=True) # Cosine similarity\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\")(c)\n",
    "\n",
    "model = Model([in1, in2], out)\n",
    "\n",
    "query_out = Model([in1], q)  # Generates query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     11921400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50)           50200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          25500       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 200)       6977200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 200)          100200      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            2           dot_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 19,074,502\n",
      "Trainable params: 7,153,102\n",
      "Non-trainable params: 11,921,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.4586\n",
      "Epoch 2/50\n",
      "2565/2565 [==============================] - 409s 159ms/step - loss: 0.3428\n",
      "Epoch 3/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.2778\n",
      "Epoch 4/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.2328\n",
      "Epoch 5/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.2012\n",
      "Epoch 6/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1778\n",
      "Epoch 7/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1607\n",
      "Epoch 8/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.1479\n",
      "Epoch 9/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1377\n",
      "Epoch 10/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1291\n",
      "Epoch 11/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1216\n",
      "Epoch 12/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.1156\n",
      "Epoch 13/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1104\n",
      "Epoch 14/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1059\n",
      "Epoch 15/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.1016\n",
      "Epoch 16/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0980\n",
      "Epoch 17/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0948\n",
      "Epoch 18/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0918\n",
      "Epoch 19/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0891\n",
      "Epoch 20/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0866\n",
      "Epoch 21/50\n",
      "2565/2565 [==============================] - 413s 161ms/step - loss: 0.0845\n",
      "Epoch 22/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0825\n",
      "Epoch 23/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0808\n",
      "Epoch 24/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0792\n",
      "Epoch 25/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0778\n",
      "Epoch 26/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0765\n",
      "Epoch 27/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0751\n",
      "Epoch 28/50\n",
      "2565/2565 [==============================] - 410s 160ms/step - loss: 0.0738\n",
      "Epoch 29/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0724\n",
      "Epoch 30/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0720\n",
      "Epoch 31/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0705\n",
      "Epoch 32/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0695\n",
      "Epoch 33/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0689\n",
      "Epoch 34/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0683\n",
      "Epoch 35/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0675\n",
      "Epoch 36/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0667\n",
      "Epoch 37/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0664\n",
      "Epoch 38/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0655\n",
      "Epoch 39/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0651\n",
      "Epoch 40/50\n",
      "2565/2565 [==============================] - 411s 160ms/step - loss: 0.0643\n",
      "Epoch 41/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0639\n",
      "Epoch 42/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0637\n",
      "Epoch 43/50\n",
      "2565/2565 [==============================] - 422s 165ms/step - loss: 0.0631\n",
      "Epoch 44/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0636\n",
      "Epoch 45/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0623\n",
      "Epoch 46/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0619\n",
      "Epoch 47/50\n",
      "2565/2565 [==============================] - 413s 161ms/step - loss: 0.0616\n",
      "Epoch 48/50\n",
      "2565/2565 [==============================] - 412s 161ms/step - loss: 0.0614\n",
      "Epoch 49/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0607\n",
      "Epoch 50/50\n",
      "2565/2565 [==============================] - 412s 160ms/step - loss: 0.0603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2edcb88cf28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = X_plots_ind.shape[0] // BATCH_SIZE\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
    "model.fit_generator(train_generator, steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    epochs = NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "model.save(\"../models/keras_model.hdf5\")\n",
    "query_out.save(\"../models/query_embedding.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vectors = movie_embedding.get_weights()[0]\n",
    "np.savetxt('../models/movie_embeddings.tsv', movie_vectors, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df[[\"Title\", \"Release Year\"]].apply(lambda x: \"%s (%i)\" %(x[0], x[1]), axis=1)\n",
    "meta = pd.DataFrame({\"Title\": titles, \"Genre\": df[\"Genre\"], \"Director\": df[\"Director\"]})\n",
    "meta.to_csv(\"../models/movie_embeddings_meta.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tf_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
