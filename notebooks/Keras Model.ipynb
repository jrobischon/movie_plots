{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinrobischon/Projects/movie_plots/movies_venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, LSTM, Dense, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wiki_movie_plots_deduped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: **\n",
    "1. Create tf-idf vectorization of documents\n",
    "2. Create generator\n",
    "    * Sample N documents\n",
    "    * Sample keywords (1 to n_max) from each document (selection prob. based on tf-idf)\n",
    "    * Sample k negative documents for each set of search terms\n",
    "    \n",
    "3. Model architecture\n",
    "    * Embedding layer for document (trained during model training)\n",
    "    * Embedding layer for keywords (use pre-trained word2vec vectors)\n",
    "    * Concatenate embeddings\n",
    "    * Output layer of 1 unit w/ binary crossentropy\n",
    "    \n",
    "\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "<img src=\"img/model_architecture.jpg\" width=400 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(x):\n",
    "    x = x.lower()\n",
    "    s_tokens =  sent_tokenize(x)\n",
    "    tokens = [TreebankWordTokenizer().tokenize(s) for s in s_tokens]\n",
    "    tokens = [[w for w in s if re.match(\"[A-Za-z]\", w) is not None] for s in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize each movie plot into a list of sentences, each containing a list of tokens\n",
    "# Return tuple containing (list of tokens, movie index)\n",
    "X_plots = []\n",
    "for i, p in enumerate(df[\"Plot\"]):\n",
    "    sentences = tokenize_text(p)\n",
    "    for s in sentences:\n",
    "        X_plots.append((s, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate plots from movie index\n",
    "x, _ = zip(*X_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train word2vec model\n",
    "wv_model = word2vec.Word2Vec(x, size=200)\n",
    "wv_model.save(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 0.8966418504714966),\n",
       " ('beast', 0.8422876596450806),\n",
       " ('demon', 0.7798564434051514),\n",
       " ('giant', 0.7623004913330078),\n",
       " ('mummy', 0.7556557655334473),\n",
       " ('werewolf', 0.7538514733314514),\n",
       " ('entity', 0.7503894567489624),\n",
       " ('sphere', 0.7459837794303894),\n",
       " ('alien', 0.7420704364776611),\n",
       " ('wolf', 0.7223528623580933)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(\"monster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7693840861320496),\n",
       " ('princess', 0.7362165451049805),\n",
       " ('empress', 0.6411824226379395),\n",
       " ('prince', 0.6059414148330688),\n",
       " ('countess', 0.591460645198822),\n",
       " ('goddess', 0.5856000781059265),\n",
       " ('emperor', 0.5829663872718811),\n",
       " ('count', 0.5658907890319824),\n",
       " ('crown', 0.5383313298225403),\n",
       " ('caliph', 0.5350780487060547)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of indices for all terms in vocab\n",
    "word2index = {\"<UNK>\": 0}\n",
    "for i, k in enumerate(wv_model.wv.index2word):\n",
    "    word2index[k] = i + 1\n",
    "    \n",
    "embedding = np.zeros((1, wv_model.wv.vectors.shape[1]))\n",
    "embedding = np.concatenate([embedding, wv_model.wv.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 64131\n",
      "Embedding matrix shape: (64131, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: %i\" %len(word2index))\n",
    "print(\"Embedding matrix shape: %s\" %str(embedding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_lookup(x):\n",
    "    try:\n",
    "        return word2index[x]\n",
    "    except KeyError:\n",
    "        return word2index[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_plots_ind = [([index_lookup(x) for x in sentence], i) for sentence, i in X_plots]\n",
    "X_plots_ind = np.array(X_plots_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create generator for training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size = 32, n_neg = 5, max_len = 100):\n",
    "    num_samples = samples.shape[0]\n",
    "    \n",
    "    ind = np.arange(num_samples)\n",
    "\n",
    "    while True:\n",
    "        samples = sk_shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            X_sent = []\n",
    "            X_movie = []\n",
    "            y_out = [] \n",
    "            \n",
    "            # Sample positive examples\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            sentences = batch_samples[:, 0]\n",
    "            movie_indices = batch_samples[:, 1]\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([1]*batch_size)\n",
    "            \n",
    "            # Sample negative examples\n",
    "            keep_indx =  np.random.choice(ind, batch_size*n_neg, replace=False)\n",
    "            neg_samples = samples[keep_indx]\n",
    "            sentences = neg_samples[:, 0]\n",
    "            movie_indices = np.repeat(movie_indices, n_neg)\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([0]*(batch_size*n_neg))\n",
    "             \n",
    "            # Pad zeros\n",
    "            X_sent = pad_sequences(X_sent, maxlen=max_len)\n",
    "             \n",
    "            yield sk_shuffle(X_sent.reshape(-1, max_len, 1), np.array(X_movie), np.array(y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = generator(X_plots_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 100, 1), (192,), (192,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sent, X_movies, y = next(t)\n",
    "\n",
    "X_sent.shape, X_movies.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movies_venv",
   "language": "python",
   "name": "movies_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
