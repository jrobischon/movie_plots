{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinrobischon/Projects/movie_plots/movies_venv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "from gensim.models import word2vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, LSTM, Dense, Bidirectional,\\\n",
    "                         concatenate, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/wiki_movie_plots_deduped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TODO: **\n",
    "1. Create tf-idf vectorization of documents\n",
    "2. Create generator\n",
    "    * Sample N documents\n",
    "    * Sample keywords (1 to n_max) from each document (selection prob. based on tf-idf)\n",
    "    * Sample k negative documents for each set of search terms\n",
    "    \n",
    "3. Model architecture\n",
    "    * Embedding layer for document (trained during model training)\n",
    "    * Embedding layer for keywords (use pre-trained word2vec vectors)\n",
    "    * Concatenate embeddings\n",
    "    * Output layer of 1 unit w/ binary crossentropy\n",
    "    \n",
    "\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "<img src=\"img/model_architecture.jpg\" width=400 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(x):\n",
    "    x = x.lower()\n",
    "    s_tokens =  sent_tokenize(x)\n",
    "    tokens = [TreebankWordTokenizer().tokenize(s) for s in s_tokens]\n",
    "    tokens = [[w for w in s if re.match(\"[A-Za-z]\", w) is not None] for s in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize each movie plot into a list of sentences, each containing a list of tokens\n",
    "# Return tuple containing (list of tokens, movie index)\n",
    "X_plots = []\n",
    "for i, p in enumerate(df[\"Plot\"]):\n",
    "    sentences = tokenize_text(p)\n",
    "    for s in sentences:\n",
    "        X_plots.append((s, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate plots from movie index\n",
    "x, _ = zip(*X_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model\n",
    "wv_model = word2vec.Word2Vec(x, size=200)\n",
    "wv_model.save(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creature', 0.8760976791381836),\n",
       " ('demon', 0.7963001132011414),\n",
       " ('beast', 0.7954497337341309),\n",
       " ('giant', 0.7572686672210693),\n",
       " ('whale', 0.7563628554344177),\n",
       " ('alien', 0.7514498233795166),\n",
       " ('monstrous', 0.7437711954116821),\n",
       " ('werewolf', 0.7436423301696777),\n",
       " ('sphere', 0.7419789433479309),\n",
       " ('dinosaur', 0.7387843132019043)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(\"monster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7539695501327515),\n",
       " ('princess', 0.7349125742912292),\n",
       " ('empress', 0.6151232719421387),\n",
       " ('prince', 0.6134809851646423),\n",
       " ('countess', 0.6115014553070068),\n",
       " ('emperor', 0.5759602189064026),\n",
       " ('goddess', 0.5578703880310059),\n",
       " ('caliph', 0.529577374458313),\n",
       " ('crown', 0.52640300989151),\n",
       " ('count', 0.5248473286628723)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of indices for all terms in vocab\n",
    "word2index = {\"<UNK>\": 0}\n",
    "for i, k in enumerate(wv_model.wv.index2word):\n",
    "    word2index[k] = i + 1\n",
    "    \n",
    "word_vectors = np.zeros((1, wv_model.wv.vectors.shape[1]))\n",
    "word_vectors = np.concatenate([word_vectors, wv_model.wv.vectors], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 64131\n",
      "Embedding matrix shape: (64131, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: %i\" %len(word2index))\n",
    "print(\"Embedding matrix shape: %s\" %str(word_vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def index_lookup(x):\n",
    "    try:\n",
    "        return word2index[x]\n",
    "    except KeyError:\n",
    "        return word2index[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_plots_ind = [([index_lookup(x) for x in sentence], i) for sentence, i in X_plots]\n",
    "X_plots_ind = np.array(X_plots_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create generator for training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size = 32, n_neg = 5, max_len = 100):\n",
    "    num_samples = samples.shape[0]\n",
    "    \n",
    "    ind = np.arange(num_samples)\n",
    "\n",
    "    while True:\n",
    "        samples = sk_shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            \n",
    "            X_sent = []\n",
    "            X_movie = []\n",
    "            y_out = [] \n",
    "            \n",
    "            # Sample positive examples\n",
    "            batch_samples = samples[offset:offset + batch_size]\n",
    "            sentences = batch_samples[:, 0]\n",
    "            movie_indices = batch_samples[:, 1]\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([1]*batch_size)\n",
    "            \n",
    "            # Sample negative examples\n",
    "            keep_indx =  np.random.choice(ind, batch_size*n_neg, replace=False)\n",
    "            neg_samples = samples[keep_indx]\n",
    "            sentences = neg_samples[:, 0]\n",
    "            movie_indices = np.repeat(movie_indices, n_neg)\n",
    "            X_sent.extend(sentences)\n",
    "            X_movie.extend(movie_indices)\n",
    "            y_out.extend([0]*(batch_size*n_neg))\n",
    "             \n",
    "            # Pad zeros\n",
    "            X_sent = pad_sequences(X_sent, maxlen=max_len)\n",
    "             \n",
    "            yield (sk_shuffle(X_sent.reshape(-1, max_len), np.array(X_movie).reshape(-1, 1)), np.array(y_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(X_plots_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 100), (192, 1), (192,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(train_generator)\n",
    "\n",
    "X[0].shape, X[1].shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "# Define query embedding layer\n",
    "d1, d2 = word_vectors.shape\n",
    "\n",
    "query_embedding = Embedding(d1, d2, weights = [word_vectors],\n",
    "               input_length = MAX_LEN,\n",
    "               trainable=False)\n",
    "\n",
    "\n",
    "# Define movie embedding layer\n",
    "movie_embedding = Embedding(df.shape[0], d2,\n",
    "                            input_length = 1,\n",
    "                            trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = Input(shape=(MAX_LEN,))\n",
    "in2 = Input(shape=(1,))\n",
    "\n",
    "q = query_embedding(in1)\n",
    "q = LSTM(10)(q)\n",
    "q = Dense(200, activation=\"relu\")(q)\n",
    "\n",
    "m = movie_embedding(in2)\n",
    "m = Flatten()(m)\n",
    "\n",
    "c = concatenate([q, m])\n",
    "\n",
    "c = Dense(50, activation = \"relu\")(c)\n",
    "out = Dense(1, activation=\"sigmoid\")(c)\n",
    "\n",
    "model = Model([in1, in2], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     12826200    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 10)           8440        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 200)       8035800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          2200        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 200)          0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           dense_1[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           20050       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            51          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,892,741\n",
      "Trainable params: 8,066,541\n",
      "Non-trainable params: 12,826,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "79/78 [==============================] - 14s 178ms/step - loss: 0.4891\n",
      "Epoch 2/100\n",
      "79/78 [==============================] - 13s 167ms/step - loss: 0.4522\n",
      "Epoch 3/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4537\n",
      "Epoch 4/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4519\n",
      "Epoch 5/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4520\n",
      "Epoch 6/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4514\n",
      "Epoch 7/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4517\n",
      "Epoch 8/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4508\n",
      "Epoch 9/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4509\n",
      "Epoch 10/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4515\n",
      "Epoch 11/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4511\n",
      "Epoch 12/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4510\n",
      "Epoch 13/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4510\n",
      "Epoch 14/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4512\n",
      "Epoch 15/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4511\n",
      "Epoch 16/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4510\n",
      "Epoch 17/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4508\n",
      "Epoch 18/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4510\n",
      "Epoch 19/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4508\n",
      "Epoch 20/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4509\n",
      "Epoch 21/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4508\n",
      "Epoch 22/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4511\n",
      "Epoch 23/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4509\n",
      "Epoch 24/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4509\n",
      "Epoch 25/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4509\n",
      "Epoch 26/100\n",
      "79/78 [==============================] - 14s 172ms/step - loss: 0.4507\n",
      "Epoch 27/100\n",
      "79/78 [==============================] - 15s 187ms/step - loss: 0.4510\n",
      "Epoch 28/100\n",
      "79/78 [==============================] - 14s 172ms/step - loss: 0.4510\n",
      "Epoch 29/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4513\n",
      "Epoch 30/100\n",
      "79/78 [==============================] - 14s 172ms/step - loss: 0.4507\n",
      "Epoch 31/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4509\n",
      "Epoch 32/100\n",
      "79/78 [==============================] - 14s 173ms/step - loss: 0.4508\n",
      "Epoch 33/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4509\n",
      "Epoch 34/100\n",
      "79/78 [==============================] - 14s 172ms/step - loss: 0.4509\n",
      "Epoch 35/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4509\n",
      "Epoch 36/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 37/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4509\n",
      "Epoch 38/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4509\n",
      "Epoch 39/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 40/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4509\n",
      "Epoch 41/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 42/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 43/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 44/100\n",
      "79/78 [==============================] - 14s 178ms/step - loss: 0.4508\n",
      "Epoch 45/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4508\n",
      "Epoch 46/100\n",
      "79/78 [==============================] - 13s 171ms/step - loss: 0.4507\n",
      "Epoch 47/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4505\n",
      "Epoch 48/100\n",
      "79/78 [==============================] - 14s 173ms/step - loss: 0.4506\n",
      "Epoch 49/100\n",
      "79/78 [==============================] - 14s 173ms/step - loss: 0.4509\n",
      "Epoch 50/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4504\n",
      "Epoch 51/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4509\n",
      "Epoch 52/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4509\n",
      "Epoch 53/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 54/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4507\n",
      "Epoch 55/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 56/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4504\n",
      "Epoch 57/100\n",
      "79/78 [==============================] - 13s 171ms/step - loss: 0.4507\n",
      "Epoch 58/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4508\n",
      "Epoch 59/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4508\n",
      "Epoch 60/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 61/100\n",
      "79/78 [==============================] - 13s 170ms/step - loss: 0.4507\n",
      "Epoch 62/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4506\n",
      "Epoch 63/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4507\n",
      "Epoch 64/100\n",
      "79/78 [==============================] - 13s 171ms/step - loss: 0.4507\n",
      "Epoch 65/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4507\n",
      "Epoch 66/100\n",
      "79/78 [==============================] - 14s 172ms/step - loss: 0.4507\n",
      "Epoch 67/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4507\n",
      "Epoch 68/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4508\n",
      "Epoch 69/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4508\n",
      "Epoch 70/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4507\n",
      "Epoch 71/100\n",
      "79/78 [==============================] - 13s 169ms/step - loss: 0.4507\n",
      "Epoch 72/100\n",
      "79/78 [==============================] - 13s 168ms/step - loss: 0.4508\n",
      "Epoch 73/100\n",
      "79/78 [==============================] - 14s 171ms/step - loss: 0.4507\n",
      "Epoch 74/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4507\n",
      "Epoch 75/100\n",
      "79/78 [==============================] - 13s 167ms/step - loss: 0.4505\n",
      "Epoch 76/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4507\n",
      "Epoch 77/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4506\n",
      "Epoch 78/100\n",
      "79/78 [==============================] - 13s 167ms/step - loss: 0.4506\n",
      "Epoch 79/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4505\n",
      "Epoch 80/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4506\n",
      "Epoch 81/100\n",
      "79/78 [==============================] - 13s 164ms/step - loss: 0.4508\n",
      "Epoch 82/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4508\n",
      "Epoch 83/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4508\n",
      "Epoch 84/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4506\n",
      "Epoch 85/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4508\n",
      "Epoch 86/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4506\n",
      "Epoch 87/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4508\n",
      "Epoch 88/100\n",
      "79/78 [==============================] - 13s 164ms/step - loss: 0.4505\n",
      "Epoch 89/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4507\n",
      "Epoch 90/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4507\n",
      "Epoch 91/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4509\n",
      "Epoch 92/100\n",
      "79/78 [==============================] - 13s 167ms/step - loss: 0.4507\n",
      "Epoch 93/100\n",
      "79/78 [==============================] - 13s 167ms/step - loss: 0.4506\n",
      "Epoch 94/100\n",
      "79/78 [==============================] - 13s 166ms/step - loss: 0.4508\n",
      "Epoch 95/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4507\n",
      "Epoch 96/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4506\n",
      "Epoch 97/100\n",
      "79/78 [==============================] - 13s 165ms/step - loss: 0.4506\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/78 [==============================] - 13s 164ms/step - loss: 0.4507\n",
      "Epoch 99/100\n",
      "79/78 [==============================] - 13s 164ms/step - loss: 0.4506\n",
      "Epoch 100/100\n",
      "79/78 [==============================] - 13s 164ms/step - loss: 0.4505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170a86eb8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "STEPS_PER_EPOCH = df.shape[0] / BATCH_SIZE\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\")\n",
    "model.fit_generator(train_generator, steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                    epochs = NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../models/keras_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_vectors = movie_embedding.get_weights()[0]\n",
    "pickle.dump(movie_vectors, open(\"../models/movie_vectors.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movies_venv",
   "language": "python",
   "name": "movies_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
